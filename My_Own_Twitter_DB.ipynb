{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shengtao Lin, Tianhe Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from tweepy) (1.2.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from tweepy) (1.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/LINHTS/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key and tokens\n",
    "#api key: bUvI6HoYH97pwjTmnMNAPTMTS\n",
    "#api secret key: Hp420vZADZ25NMR0sqEf4eVlYXVtUsyL4AhwgdAxTf2ssbr0in\n",
    "#access token: 1190435999551303680-vK990ORPT4tj7KU1TTkfOtARxNzLfz\n",
    "#Access Token Secret: 6KWkz1UYuCPvvGKk22AvMumRpwlA2kO8DlIVZ5KwpQtTK\n",
    "\n",
    "access_token = \"1190435999551303680-vK990ORPT4tj7KU1TTkfOtARxNzLfz\"\n",
    "access_token_secret = \"6KWkz1UYuCPvvGKk22AvMumRpwlA2kO8DlIVZ5KwpQtTK\"\n",
    "consumer_key = \"bUvI6HoYH97pwjTmnMNAPTMTS\"\n",
    "consumer_secret = \"Hp420vZADZ25NMR0sqEf4eVlYXVtUsyL4AhwgdAxTf2ssbr0in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "default_stdout = sys.stdout\n",
    "sys.stdout = open('AI-out', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "End",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m End\n"
     ]
    }
   ],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "    t_count=0\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        print (data)\n",
    "        self.t_count += 1\n",
    "        \n",
    "        #stop by \n",
    "        if self.t_count >=12000:\n",
    "            sys.exit(\"End\")\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    stream.filter(track=['#ArtificialIntelligence'],languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = default_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "count=0\n",
    "with open(\"AI-out\", \"r\") as f1:\n",
    "    for line in f1:\n",
    "        count+=1\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # if you want to see a specific field, you can print it. \n",
    "            #If your file is big, there may be too many of these printed\n",
    "            #print(data['text']) \n",
    "        except:\n",
    "            continue\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataBase create/insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database import\n",
    "import mysql.connector\n",
    "import pymongo\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydbsql = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"789456565632\",\n",
    "  database=\"TwitterDB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = mydbsql.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "1007 (HY000): Can't create database 'twitterdb'; database exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMySQLInterfaceError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\philw\\miniconda3\\lib\\site-packages\\mysql\\connector\\connection_cext.py\u001b[0m in \u001b[0;36mcmd_query\u001b[1;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m             self._cmysql.query(query,\n\u001b[0m\u001b[0;32m    507\u001b[0m                                \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffered\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMySQLInterfaceError\u001b[0m: Can't create database 'twitterdb'; database exists",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7b6cbe716c2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#create DATABASE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmycursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CREATE DATABASE TwitterDB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\philw\\miniconda3\\lib\\site-packages\\mysql\\connector\\cursor_cext.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, operation, params, multi)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             result = self._cnx.cmd_query(stmt, raw=self._raw,\n\u001b[0m\u001b[0;32m    270\u001b[0m                                          \u001b[0mbuffered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffered\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                                          raw_as_string=self._raw_as_string)\n",
      "\u001b[1;32mc:\\users\\philw\\miniconda3\\lib\\site-packages\\mysql\\connector\\connection_cext.py\u001b[0m in \u001b[0;36mcmd_query\u001b[1;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[0;32m    508\u001b[0m                                raw_as_string=raw_as_string)\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMySQLInterfaceError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m             raise errors.get_mysql_exception(exc.errno, msg=exc.msg,\n\u001b[0m\u001b[0;32m    511\u001b[0m                                              sqlstate=exc.sqlstate)\n\u001b[0;32m    512\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: 1007 (HY000): Can't create database 'twitterdb'; database exists"
     ]
    }
   ],
   "source": [
    "#create DATABASE\n",
    "mycursor.execute(\"CREATE DATABASE TwitterDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "#mycursor.execute(\"USE TwitterDB\")\n",
    "mycursor.execute(\"CREATE TABLE IF NOT EXISTS user(\"+\n",
    "                 \"user_id VARCHAR(255) PRIMARY KEY,\"+\n",
    "                \"name VARCHAR(255),\"+\n",
    "                \"screen_name VARCHAR(255),\"+\n",
    "                \"location VARCHAR(255),\"+\n",
    "                \"description VARCHAR(255),\"+\n",
    "                \"followers_count INT NOT NULL,\"+\n",
    "                \"friends_count INT NOT NULL,\"+\n",
    "                \"listed_count INT NOT NULL,\"+\n",
    "                \"favourites_count INT NOT NULL,\"+\n",
    "                \"statuses_count INT NOT NULL,\"+\n",
    "                \"created_at DATETIME NOT NULL);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"CREATE TABLE IF NOT EXISTS post(\"+\n",
    "                 \"user_id VARCHAR(255) NOT NULL,\"+\n",
    "                 \"post_id VARCHAR(255) NOT NULL,\"+\n",
    "                 \"created_at DATETIME NOT NULL,\"+\n",
    "                 \"PRIMARY KEY (user_id, post_id),\"+\n",
    "                 \"FOREIGN KEY (user_id) REFERENCES user(user_id));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop user table for test\n",
    "sql = \"DROP TABLE post\"\n",
    "mycursor.execute(sql)\n",
    "sql = \"DROP TABLE user\"\n",
    "mycursor.execute(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MongoDB\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create database\n",
    "mydbmongo = myclient[\"TwitterDB\"]\n",
    "#create collection\n",
    "mycol = mydbmongo[\"Tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-58ba63afac64>:91: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  if mycol.find({'_id': tweet_id}).limit(1).count() > 0:\n"
     ]
    }
   ],
   "source": [
    "#start loading\n",
    "#load function first\n",
    "\n",
    "with open(\"AI-out\", \"r\") as f1:\n",
    "    for line in f1:\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            #print(data['id'])\n",
    "            ProcessData(data) \n",
    "            \n",
    "        except OSError as err:\n",
    "            print(\"OS error: {0}\".format(err))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        except:\n",
    "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(data):\n",
    "    \n",
    "#-----------user infomation----------\n",
    "    #tweet user\n",
    "    if checkExist(data['user'][\"id\"]):\n",
    "        #already exist \n",
    "        updateUser(data['user'])\n",
    "    else:\n",
    "        #new user\n",
    "        createNewUser(data['user']) \n",
    "        \n",
    "        \n",
    "    #retweet user\n",
    "    if \"retweeted_status\" in data:\n",
    "        if checkExist(data['retweeted_status']['user'][\"id\"]):\n",
    "            #already exist\n",
    "            updateUser(data['retweeted_status']['user'])\n",
    "        else:\n",
    "            #new user\n",
    "            createNewUser(data['retweeted_status']['user'])\n",
    "            \n",
    "#------------post ----------------\n",
    "    newPost(data)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkExist(user_id):\n",
    "    idquery= \"SELECT COUNT(1) FROM user WHERE user_id = \"+str(user_id)+\";\"\n",
    "    mycursor.execute(idquery)\n",
    "    myresult = mycursor.fetchall()\n",
    "\n",
    "    if(myresult[0][0]!=0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewUser(data):\n",
    "    user_id = data[\"id\"]\n",
    "    name = data[\"name\"]\n",
    "    screen_name=data[\"screen_name\"]\n",
    "    location = data[\"location\"]\n",
    "    description = data[\"description\"]\n",
    "    followers_count = data[\"followers_count\"]\n",
    "    friends_count = data[\"friends_count\"]\n",
    "    listed_count = data[\"listed_count\"]\n",
    "    favourites_count = data[\"favourites_count\"]\n",
    "    statuses_count = data[\"statuses_count\"]\n",
    "    \n",
    "    new_datetime = datetime.strftime(datetime.strptime(data[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "    created_at = new_datetime\n",
    "    \n",
    "    sql = (\"INSERT INTO user (user_id, name, screen_name, location,\"+\n",
    "    \" description, followers_count, friends_count, listed_count, favourites_count, statuses_count, created_at)\"+\n",
    "    \" VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\")\n",
    "    val=(user_id,name,screen_name,location,description,int(followers_count)\n",
    "         ,int(friends_count),int(listed_count),int(favourites_count),int(statuses_count),created_at)\n",
    " \n",
    "    mycursor.execute(sql, val)\n",
    "    mydbsql.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateUser(data):\n",
    "    user_id = data[\"id\"]\n",
    "    name = data[\"name\"]\n",
    "    screen_name=data[\"screen_name\"]\n",
    "    location = data[\"location\"]\n",
    "    description = data[\"description\"]\n",
    "    followers_count = data[\"followers_count\"]\n",
    "    friends_count = data[\"friends_count\"]\n",
    "    listed_count = data[\"listed_count\"]\n",
    "    favourites_count = data[\"favourites_count\"]\n",
    "    statuses_count = data[\"statuses_count\"]\n",
    "    \n",
    "    sql = (\"UPDATE user SET name = %s, \"+\n",
    "    \"screen_name= %s, \"+\n",
    "    \"location= %s, \"+\n",
    "    \"description= %s, \"+\n",
    "    \"followers_count= %s, \"+\n",
    "    \"friends_count= %s, \"+\n",
    "    \"listed_count= %s, \"+\n",
    "    \"favourites_count= %s, \"+\n",
    "    \"statuses_count= %s \"+\n",
    "    \"WHERE user_id = %s\")\n",
    "    \n",
    "    \n",
    "    val = (name,screen_name,location,description,int(followers_count),\n",
    "           int(friends_count),int(listed_count),int(favourites_count),int(statuses_count),user_id)\n",
    "    #print(sql)\n",
    "    mycursor.execute(sql, val)\n",
    "    mydbsql.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newPost(data):\n",
    "    if \"retweeted_status\" in data:\n",
    "        # it is a RT\n",
    "        RT = 1\n",
    "        \n",
    "        #for tweet\n",
    "        if \"extended_tweet\" in data:\n",
    "            #for tweet mongoDB\n",
    "            tweet_id = data['id']\n",
    "            text_full = data['extended_tweet']['full_text']\n",
    "            text=text_full.partition('RT')[0]\n",
    "            \n",
    "            hashtag = []\n",
    "            if(\"extended_tweet\" in data['retweeted_status']):\n",
    "                for ht in (data['retweeted_status']['extended_tweet']['entities']['hashtags']):\n",
    "                    hashtag.append(ht['text'])\n",
    "            else:\n",
    "                for ht in (data['retweeted_status']['entities']['hashtags']):\n",
    "                    hashtag.append(ht['text'])\n",
    "        \n",
    "            new_datetime = datetime.strftime(datetime.strptime(data[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "            created_at = new_datetime\n",
    "        \n",
    "            quote_count=data['quote_count']\n",
    "            reply_count=data['reply_count']\n",
    "            retweet_count=data['retweet_count']\n",
    "            favorite_count=data['favorite_count']\n",
    "            \n",
    "            user_id=data['user']['id']\n",
    "            user_screen_name=data['user']['screen_name']\n",
    "            \n",
    "            rt_id=data['retweeted_status']['id']\n",
    "            \n",
    "        \n",
    "            mydict = { \"_id\": tweet_id,\"RT\":RT,\"text\":text,\"hashtag\":hashtag,\"created_at\":created_at,\n",
    "                      \"quote_count\":quote_count,\"reply_count\":reply_count,\"retweet_count\":retweet_count,\n",
    "                      \"favorite_count\":favorite_count,\"user_screen_name\":user_screen_name,\"rt_id\":rt_id}\n",
    "        \n",
    "            x = mycol.insert_one(mydict)\n",
    "        \n",
    "            #for post table mySQL\n",
    "        \n",
    "            sql = (\"INSERT INTO post (user_id, post_id, created_at) VALUES (%s, %s, %s)\")\n",
    "            val=(user_id,tweet_id,created_at)\n",
    "        \n",
    "            mycursor.execute(sql, val)\n",
    "            mydbsql.commit()\n",
    "        \n",
    "        else:\n",
    "            #for tweet mongoDB\n",
    "            tweet_id = data['id']\n",
    "            text_full = data['text']\n",
    "            text=text_full.partition('RT')[0]\n",
    "            \n",
    "            hashtag = []\n",
    "            if(\"extended_tweet\" in data['retweeted_status']):\n",
    "                for ht in (data['retweeted_status']['extended_tweet']['entities']['hashtags']):\n",
    "                    hashtag.append(ht['text'])\n",
    "            else:\n",
    "                for ht in (data['retweeted_status']['entities']['hashtags']):\n",
    "                    hashtag.append(ht['text'])\n",
    "        \n",
    "            new_datetime = datetime.strftime(datetime.strptime(data[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "            created_at = new_datetime\n",
    "        \n",
    "            quote_count=data['quote_count']\n",
    "            reply_count=data['reply_count']\n",
    "            retweet_count=data['retweet_count']\n",
    "            favorite_count=data['favorite_count']\n",
    "            \n",
    "            user_screen_name=data['user']['screen_name']\n",
    "            rt_id=data['retweeted_status']['id']\n",
    "        \n",
    "            mydict = { \"_id\": tweet_id,\"RT\":RT,\"text\":text,\"hashtag\":hashtag,\"created_at\":created_at,\n",
    "                      \"quote_count\":quote_count,\"reply_count\":reply_count,\"retweet_count\":retweet_count,\n",
    "                      \"favorite_count\":favorite_count,\"user_screen_name\":user_screen_name,\"rt_id\":rt_id}\n",
    "        \n",
    "            x = mycol.insert_one(mydict)\n",
    "        \n",
    "            #for post table mySQL\n",
    "            user_id=data['user']['id']\n",
    "        \n",
    "            sql = (\"INSERT INTO post (user_id, post_id, created_at) VALUES (%s, %s, %s)\")\n",
    "            val=(user_id,tweet_id,created_at)\n",
    "        \n",
    "            mycursor.execute(sql, val)\n",
    "            mydbsql.commit()\n",
    "        \n",
    "        #for retweet\n",
    "        tweet_id = data['retweeted_status']['id']\n",
    "        if mycol.find({'_id': tweet_id}).limit(1).count() > 0:\n",
    "            # existed retweet\n",
    "            pass\n",
    "        else:\n",
    "            # new retweet\n",
    "            #print(data['retweeted_status'])\n",
    "            insertNewPost(data['retweeted_status'])\n",
    "        \n",
    "    else:\n",
    "        # not a RT\n",
    "        insertNewPost(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertNewPost(data):\n",
    "    if \"extended_tweet\" in data:\n",
    "        #for tweet mongoDB\n",
    "        tweet_id = data['id']\n",
    "        RT = 0\n",
    "        text = data['extended_tweet']['full_text']\n",
    "        hashtag = []\n",
    "        for ht in (data['extended_tweet']['entities']['hashtags']):\n",
    "            hashtag.append(ht['text'])\n",
    "        \n",
    "        new_datetime = datetime.strftime(datetime.strptime(data[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "        created_at = new_datetime\n",
    "    \n",
    "        quote_count=data['quote_count']\n",
    "        reply_count=data['reply_count']\n",
    "        retweet_count=data['retweet_count']\n",
    "        favorite_count=data['favorite_count']\n",
    "            \n",
    "        user_id=data['user']['id']\n",
    "        user_screen_name=data['user']['screen_name']\n",
    "        \n",
    "        mydict = { \"_id\": tweet_id,\"RT\":RT,\"text\":text,\"hashtag\":hashtag,\"created_at\":created_at,\n",
    "                  \"quote_count\":quote_count,\"reply_count\":reply_count,\"retweet_count\":retweet_count,\n",
    "                  \"favorite_count\":favorite_count,\"user_screen_name\":user_screen_name}\n",
    "        \n",
    "        x = mycol.insert_one(mydict)\n",
    "        \n",
    "        #for post table mySQL\n",
    "        \n",
    "        sql = (\"INSERT INTO post (user_id, post_id, created_at) VALUES (%s, %s, %s)\")\n",
    "        val=(user_id,tweet_id,created_at)\n",
    "    \n",
    "        mycursor.execute(sql, val)\n",
    "        mydbsql.commit()\n",
    "        \n",
    "    else:\n",
    "        #for tweet mongoDB\n",
    "        tweet_id = data['id']\n",
    "        RT = 0\n",
    "        text = data['text']\n",
    "        hashtag = []\n",
    "        for ht in (data['entities']['hashtags']):\n",
    "            hashtag.append(ht['text'])\n",
    "        \n",
    "        new_datetime = datetime.strftime(datetime.strptime(data[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "        created_at = new_datetime\n",
    "        \n",
    "        quote_count=data['quote_count']\n",
    "        reply_count=data['reply_count']\n",
    "        retweet_count=data['retweet_count']\n",
    "        favorite_count=data['favorite_count']\n",
    "            \n",
    "        user_screen_name=data['user']['screen_name']\n",
    "        \n",
    "        mydict = { \"_id\": tweet_id,\"RT\":RT,\"text\":text,\"hashtag\":hashtag,\"created_at\":created_at,\n",
    "                  \"quote_count\":quote_count,\"reply_count\":reply_count,\"retweet_count\":retweet_count,\n",
    "                  \"favorite_count\":favorite_count,\"user_screen_name\":user_screen_name}\n",
    "        \n",
    "        x = mycol.insert_one(mydict)\n",
    "        \n",
    "        #for post table mySQL\n",
    "        user_id=data['user']['id']\n",
    "        \n",
    "        sql = (\"INSERT INTO post (user_id, post_id, created_at) VALUES (%s, %s, %s)\")\n",
    "        val=(user_id,tweet_id,created_at)\n",
    "        \n",
    "        mycursor.execute(sql, val)\n",
    "        mydbsql.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-70e28d54814a>:7: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global cache \n",
    "cache = Cache(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPost(posts, key):\n",
    "    result=[]\n",
    "    for x in posts:\n",
    "        text=x['text']\n",
    "        created_at=x['created_at']\n",
    "        quote_count=x['quote_count']\n",
    "        reply_count=x['reply_count']\n",
    "        retweet_count=x['retweet_count']\n",
    "        favorite_count=x['favorite_count']\n",
    "        user_screen_name=x['user_screen_name']\n",
    "        RT=x['RT']\n",
    "        if RT==1:\n",
    "            rtid=x['rt_id']\n",
    "            myquery = { \"_id\": rtid }\n",
    "            mydoc = mycol.find(myquery)\n",
    "            for y in mydoc:\n",
    "                rt_text=y['text']\n",
    "                rt_name=y['user_screen_name']\n",
    "            text = str(text)+(\" @\")+str(rt_name)+(\" \")+str(rt_text)\n",
    "        result.append([user_screen_name,text,created_at,quote_count,reply_count,retweet_count,favorite_count])\n",
    "    df=pd.DataFrame(result,columns =[\"name\",\"text\",\"created_at\",\"#quote\",\"#reply\",\"#retweet\",\"#favorite\"])\n",
    "    \n",
    "    #setting cache\n",
    "    cache.put(key, df)\n",
    "    \n",
    "    l=len(df.index)\n",
    "    i=0\n",
    "    if l<=5:\n",
    "        print(df)\n",
    "    else:\n",
    "        while i+5<l:\n",
    "            print(df[i:i+5])\n",
    "            i=i+5\n",
    "            statement = input(\"Displaying 5 results with total \"+str(l)+\n",
    "                             \" results, enter 0 to exit, enter any other key to see more.\\n\")\n",
    "            if statement == '0':\n",
    "                break\n",
    "        print(df[i:l])\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchWord(word):\n",
    "    df = cache.get(word)\n",
    "    if type(df) != int:\n",
    "        cache.put(word, df)\n",
    "        print(df)\n",
    "    else:\n",
    "        t0 = time.time()\n",
    "        result=[]\n",
    "        que=\".*\"+str(word)+\".*\"\n",
    "        #regular expression search\n",
    "        myquery = {\"text\": {\"$regex\" : re.compile(que)}}\n",
    "\n",
    "        mydoc = mycol.find(myquery).sort(\"retweet_count\", -1)\n",
    "        for y in mydoc:\n",
    "            result.append(y)\n",
    "\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "        print(\"search time used: \"+ str(total))    \n",
    "        processPost(result, word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=None\n",
    "def searchTag(hashtag):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    df = cache.get(hashtag)\n",
    "    if type(df) != int:\n",
    "        cache.put(hashtag, df)\n",
    "        #show only 5 ouputs if df includes more than 5 \n",
    "        l=len(df.index)\n",
    "        i=0\n",
    "        if l<=5:\n",
    "            print(df)\n",
    "        else:\n",
    "            while i+5<l:\n",
    "                print(df[i:i+5])\n",
    "                i=i+5\n",
    "                statement = input(\"Displaying 5 results with total \"+str(l)+\n",
    "                                 \" results, enter 0 to exit, enter any other key to see more.\\n\")\n",
    "                if statement == '0':\n",
    "                    break\n",
    "            print(df[i:l])\n",
    "    \n",
    "    else:\n",
    "        res = []\n",
    "        global temp\n",
    "        myquery = {\"hashtag\": hashtag}\n",
    "        mydoc = mycol.find(myquery).sort(\"retweet_count\", -1)\n",
    "\n",
    "        for y in mydoc:\n",
    "            res.append(y)\n",
    "\n",
    "        if res:\n",
    "            t1 = time.time()\n",
    "            total = t1-t0\n",
    "            print(\"search time used: \"+ str(total))\n",
    "            temp=res\n",
    "            processPost(res, hashtag)\n",
    "        else:\n",
    "            print(\"No tweets relates to this hashtag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4910d49d8237>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AI\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "cache.get(\"AI\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(date_text):\n",
    "    try:\n",
    "        datetime.strptime(date_text, \"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Incorrect data format, should be YYYY-MM-DD\")\n",
    "        \n",
    "def searchtime(beginning, ending):\n",
    "    t0 = time.time()\n",
    "    res = []\n",
    "    \n",
    "    #validate if user input correct format of date\n",
    "    validate(beginning)\n",
    "    validate(ending)\n",
    "    \n",
    "    beginning = beginning + str(\" 00:00:00\")\n",
    "    ending = ending + str(\" 23:59:59\")\n",
    "    myquery = {\"created_at\": {\n",
    "        \"$gte\": beginning,\n",
    "        \"$lte\": ending\n",
    "    } \n",
    "    }\n",
    "    mydoc = mycol.find(myquery).sort(\"created_at\", -1)\n",
    "    \n",
    "    for i in mydoc:\n",
    "        res.append(i)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    print(\"search time used: \"+ str(total))\n",
    "    processPost(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchUser(name):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    name_1 = name + \"1\"\n",
    "    df = cache.get(name_1)\n",
    "    if type(df) != int:\n",
    "        cache.put(name_1, df)\n",
    "        print(df)\n",
    "    else:\n",
    "        result=[]\n",
    "        #get user info\n",
    "        sql = \"SELECT * FROM user WHERE (user_id = %s) OR (name = %s) OR (screen_name = %s);\"\n",
    "        val=(name,name,name)\n",
    "        mycursor.execute(sql, val)\n",
    "        myresult = mycursor.fetchall()\n",
    "        x=myresult[0]\n",
    "        df = pd.DataFrame([list(x),], columns =[\"id\", \"name\", \"screen_name\", \"location\",\n",
    "                                                \"description\",\"followers_count\",\"friends_count\",\n",
    "                                                \"listed_count\",\"favourites_count\",\"statuses_count\",\"created_at\"])\n",
    "        cache.put(name_1, df)\n",
    "        print(df)\n",
    "    \n",
    "    #get user posts\n",
    "    name_2 = name + \"2\"\n",
    "    df2 = cache.get(name_2)\n",
    "    if type(df2) != int:\n",
    "        cache.put(name_2, df2)\n",
    "        print(df2)\n",
    "    else:\n",
    "        user_id=str(x[0])\n",
    "        sql = \"SELECT * FROM post WHERE (user_id= %s) ORDER BY created_at DESC;\"\n",
    "        val = (user_id,)\n",
    "        mycursor.execute(sql, val)\n",
    "        myresult = mycursor.fetchall()\n",
    "\n",
    "        for x in myresult:\n",
    "            myquery = { \"_id\": int(x[1]) }\n",
    "            mydoc = mycol.find(myquery)\n",
    "            for y in mydoc:\n",
    "                result.append(y)\n",
    "\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "        print(\"search time used: \"+ str(total))\n",
    "        processPost(result, name_2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    while True:\n",
    "        statement = input(\"For user search, enter 1\\n\"+\"For tweet search, enter 2\\n\"+\n",
    "                          \"For hashtag search, enter 3\\n\"+\"For time range search, enter 4\\n\"+\n",
    "                          \"To exit, enter 0\\n\")\n",
    "        if(statement=='0'):\n",
    "            print(\"Exit\")\n",
    "            break\n",
    "        elif(statement=='1'):\n",
    "            statement = input(\"Please enter the user name, screen name, or id:\\n\")\n",
    "            searchUser(statement)\n",
    "            break\n",
    "        elif(statement=='2'):\n",
    "            statement = input(\"Please enter the keyword:\\n\")\n",
    "            searchWord(statement)\n",
    "            break\n",
    "        elif(statement=='3'):\n",
    "            statement = input(\"Please enter the hashtag:\\n\")\n",
    "            searchTag(statement)\n",
    "            break\n",
    "        elif(statement=='4'):\n",
    "            statement1 = input(\"Please enter the begine time in format YYYY-MM-DD:\\n\")\n",
    "            statement2 = input(\"Please enter the end time in format YYYY-MM-DD:\\n\")\n",
    "            searchtime(statement1,statement2)\n",
    "            break\n",
    "        else:\n",
    "            print(\"invalid input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For user search, enter 1\n",
      "For tweet search, enter 2\n",
      "For hashtag search, enter 3\n",
      "For time range search, enter 4\n",
      "To exit, enter 0\n",
      "2\n",
      "Please enter the keyword:\n",
      "Dr\n",
      "search time used: 0.025931358337402344\n",
      "             name  \\\n",
      "0  BenjaminP3ters   \n",
      "1  IainLJBrown      \n",
      "2  BenjaminP3ters   \n",
      "3  IainLJBrown      \n",
      "4  IainLJBrown      \n",
      "\n",
      "                                                                                                                                                                                                                                                                                          text  \\\n",
      "0  Drive-thrus using artificial intelligence to predict orders | Connect the Dots - https://t.co/mHZLGyNWlY\\n\\nRead more here: https://t.co/MCpgw1okwe\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT              \n",
      "1  CSG Conversational Artificial Intelligence Recognized Globally for Driving a Personalized and Engaging Customer Experience - Yahoo Finance\\n\\nRead more here: https://t.co/vU6XPPAQox\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLe…   \n",
      "2  CSG Conversational Artificial Intelligence Recognized Globally for Driving a Personalized and Engaging Customer Experience - Yahoo Finance\\n\\nRead more here: https://t.co/xS1erLs5s7\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLe…   \n",
      "3  AI-Driven Decision Making: How Artificial Intelligence is Reshaping Asset Management - Utility Dive\\n\\nRead more here: https://t.co/pgn7zzKJbN\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                   \n",
      "4  Artificial Intelligence and Machine Learning Drive Advancements in Banking Technology - Computerworld\\n\\nRead more here: https://t.co/vq5DQ8p2YU\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                 \n",
      "\n",
      "            created_at  #quote  #reply  #retweet  #favorite  \n",
      "0  2021-03-10 08:38:36  0       0       624       59         \n",
      "1  2021-03-12 09:53:05  0       0       618       54         \n",
      "2  2021-03-12 09:38:07  0       0       607       50         \n",
      "3  2021-03-11 08:53:25  0       0       594       47         \n",
      "4  2021-03-19 07:53:23  1       0       591       57         \n",
      "Displaying 5 results with total 77 results, enter 0 to exit, enter any other key to see more.\n",
      "0\n",
      "               name  \\\n",
      "5   IainLJBrown       \n",
      "6   BenjaminP3ters    \n",
      "7   BenjaminP3ters    \n",
      "8   BenjaminP3ters    \n",
      "9   IainLJBrown       \n",
      "10  FabienBrodie      \n",
      "11  FabienBrodie      \n",
      "12  SuzanneC0leman    \n",
      "13  Samu3lR0y         \n",
      "14  Samu3lR0y         \n",
      "15  FabienBrodie      \n",
      "16  FabienBrodie      \n",
      "17  SuzanneC0leman    \n",
      "18  MarcoPark21       \n",
      "19  FabienBrodie      \n",
      "20  FabienBrodie      \n",
      "21  FabienBrodie      \n",
      "22  FabienBrodie      \n",
      "23  FabienBrodie      \n",
      "24  Samu3lR0y         \n",
      "25  SuzanneC0leman    \n",
      "26  IainLJBrown       \n",
      "27  FabienBrodie      \n",
      "28  FabienBrodie      \n",
      "29  FabienBrodie      \n",
      "30  J3nTyrell         \n",
      "31  FabienBrodie      \n",
      "32  SuzanneC0leman    \n",
      "33  FabienBrodie      \n",
      "34  SuzanneC0leman    \n",
      "35  BenjaminP3ters    \n",
      "36  IainLJBrown       \n",
      "37  FabienBrodie      \n",
      "38  FabienBrodie      \n",
      "39  BenjaminP3ters    \n",
      "40  IainLJBrown       \n",
      "41  BenjaminP3ters    \n",
      "42  IainLJBrown       \n",
      "43  FabienBrodie      \n",
      "44  SuzanneC0leman    \n",
      "45  BenjaminP3ters    \n",
      "46  SuzanneC0leman    \n",
      "47  BenjaminP3ters    \n",
      "48  BenjaminP3ters    \n",
      "49  FabienBrodie      \n",
      "50  IainLJBrown       \n",
      "51  SuzanneC0leman    \n",
      "52  Fabriziobustama   \n",
      "53  BenjaminP3ters    \n",
      "54  Ronald_vanLoon    \n",
      "55  FabienBrodie      \n",
      "56  Fabriziobustama   \n",
      "57  SpirosMargaris    \n",
      "58  globalupstream    \n",
      "59  CEO_AISOMA        \n",
      "60  genericgranola    \n",
      "61  genericgranola    \n",
      "62  genericgranola    \n",
      "63  genericgranola    \n",
      "64  genericgranola    \n",
      "65  Fabriziobustama   \n",
      "66  EdKwedar          \n",
      "67  future_of_AI      \n",
      "68  GMLTweet          \n",
      "69  MinaChan77        \n",
      "70  BatterySolid      \n",
      "71  Rock_n_Heavy      \n",
      "72  genericgranola    \n",
      "73  ZEN_pub           \n",
      "74  TihaLong          \n",
      "75  Eli_Krumova       \n",
      "76  techjunkiejh      \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                     text  \\\n",
      "5   Drive-thrus using artificial intelligence to predict orders | Connect the Dots - https://t.co/Yn0ujNLPvw\\n\\nRead more here: https://t.co/78qGuSjz9J\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                        \n",
      "6   AI-Driven Decision Making: How Artificial Intelligence is Reshaping Asset Management - Utility Dive\\n\\nRead more here: https://t.co/4Y4FFySuWs\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT                                             \n",
      "7   How Artificial Intelligence Can Drive Healthcare Innovation - Barron's\\n\\nRead more here: https://t.co/w6xlwNCCZh\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT                                                                          \n",
      "8   Artificial Intelligence and Machine Learning Drive Advancements in Banking Technology - Computerworld\\n\\nRead more here: https://t.co/SPAlsQYJZq\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT                                           \n",
      "9   How Artificial Intelligence Can Drive Healthcare Innovation - Barron's\\n\\nRead more here: https://t.co/79Vyf7IEwl\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                          \n",
      "10  Danish Regulator Drafts Criteria For AI, Machine Learning Algorithms - Pink Sheet\\n\\nRead more here: https://t.co/iP4RssAvxw\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                      \n",
      "11  Danish Regulator Drafts Criteria For AI, Machine Learning Algorithms - Pink Sheet\\n\\nRead more here: https://t.co/iP4RssAvxw\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                      \n",
      "12  AI-Driven Decision Making: How Artificial Intelligence is Reshaping Asset Management - Utility Dive\\n\\nRead more here: https://t.co/5cXizKjDGV\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                    \n",
      "13  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - Odessa American\\n\\nRead more here: https://t.co/0bQX3N85ko\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                            \n",
      "14  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - Yahoo Finance\\n\\nRead more here: https://t.co/Oi9HIXcoou\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                              \n",
      "15  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - Maryville Daily Times\\n\\nRead more here: https://t.co/LBazeCic4T\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                      \n",
      "16  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - Business Wire\\n\\nRead more here: https://t.co/kIaOJN4stG\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                              \n",
      "17  Artificial Intelligence and Machine Learning Drive Advancements in Banking Technology - Computerworld\\n\\nRead more here: https://t.co/vjyyLAEajv\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                  \n",
      "18  Adding Deep Learning to Ultra-Low Dose CT Drastically Reduces Exposure for Emphysema Patients - Diagnostic Imaging\\n\\nRead more here: https://t.co/qhUbFIfxY0\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                     \n",
      "19  Machine learning calculates affinities of drug candidates and targets - Drug Target Review\\n\\nRead more here: https://t.co/ltCh704vD6\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                             \n",
      "20  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - Valdosta Daily Times\\n\\nRead more here: https://t.co/QmboxBvQHv\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                       \n",
      "21  Machine Learning Technique Could Accelerate Drug Discovery - https://t.co/M8pNExXXMN\\n\\nRead more here: https://t.co/ShNxxKdVCO\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                   \n",
      "22  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - https://t.co/Ze3tdGJQSZ\\n\\nRead more here: https://t.co/wdkQP8qRlx\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                    \n",
      "23  Artificial Intelligence and Machine Learning Drive Advancements in Banking Technology - Computerworld\\n\\nRead more here: https://t.co/yRo0KqCbZO\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                  \n",
      "24  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - Maryville Daily Times\\n\\nRead more here: https://t.co/6nX74myz7k\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                      \n",
      "25  How Artificial Intelligence Can Drive Healthcare Innovation - Barron's\\n\\nRead more here: https://t.co/ye88ydMEh3\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                                 \n",
      "26  How Artificial Intelligence Can Drive Healthcare Innovation - Barron's\\n\\nRead more here: https://t.co/79Vyf7IEwl\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                          \n",
      "27  Dataiku and UAVIA Collaboration Successfully Deploys Machine Learning Models for Edge Computing on Drones - Yahoo Finance\\n\\nRead more here: https://t.co/6ChI4cVH7Y\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                              \n",
      "28  HMS Researchers Use Machine Learning to Recommend Drugs for Alzheimer's Disease Clinical Trials | News - Harvard Crimson\\n\\nRead more here: https://t.co/DGmZCANvdK\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                               \n",
      "29  Smart Reviews Platform Launched To Help Root Out Dropshipped “Burner Brands” - MarTech Series\\n\\nRead more here: https://t.co/hIXH8OQKXH\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                          \n",
      "30  Adding Deep Learning to Ultra-Low Dose CT Drastically Reduces Exposure for Emphysema Patients - Diagnostic Imaging\\n\\nRead more here: https://t.co/gATMzX0gPo\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                     \n",
      "31  BMW's iDrive 8 helps drivers using machine learning and natural language processing - Engadget\\n\\nRead more here: https://t.co/0zmMbhCnP6\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                         \n",
      "32  CSG Conversational Artificial Intelligence Recognized Globally for Driving a Personalized and Engaging Customer Experience - Yahoo Finance\\n\\nRead more here: https://t.co/rHYdY7FZlL\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                             \n",
      "33  Quickly Calculating Drug–Target Binding Affinity With Machine Learning - Technology Networks\\n\\nRead more here: https://t.co/UPHfDzlWQF\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                           \n",
      "34  CSG Conversational Artificial Intelligence Recognized Globally for Driving a Personalized and Engaging Customer Experience - Yahoo Finance\\n\\nRead more here: https://t.co/rHYdY7FZlL\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                             \n",
      "35  How Artificial Intelligence Can Drive Healthcare Innovation - Barron's\\n\\nRead more here: https://t.co/w6xlwNCCZh\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT                                                                          \n",
      "36  CamerEye Introduces First Artificial Intelligence Smart Fence and Pool Safety Ecosystem for Faster Distress and Near-Drowning Detection to Help Save Lives - PRNewswire\\n\\nRead more here: https://t.co/tRxA5JmmFb\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #Mac…                             \n",
      "37  BMW's iDrive 8 helps drivers using machine learning and natural language processing - Yahoo Tech\\n\\nRead more here: https://t.co/IrLZwtjcaM\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                       \n",
      "38  This Boston Based Startup is Applying Machine Learning-Anchored Computation to Enhance Drug Discovery and Development - MarkTechPost\\n\\nRead more here: https://t.co/mq2zRngBXz\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                   \n",
      "39  Using Artificial Intelligence To Discover New Drug Treatments: Science Next - WRKF\\n\\nRead more here: https://t.co/dQL4anhP5E\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT                                                              \n",
      "40  Using Artificial Intelligence To Discover New Drug Treatments: Science Next - WRKF\\n\\nRead more here: https://t.co/v03TaOrJxF\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                              \n",
      "41  Dr. Abdalla Kablan talks artificial intelligence - News3LV\\n\\nRead more here: https://t.co/HN9GhP9jDY\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT                                                                                      \n",
      "42  Dr. Abdalla Kablan talks artificial intelligence - News3LV\\n\\nRead more here: https://t.co/Ss1cVgDBRj\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                      \n",
      "43  Machine Learning Algorithm Predicts Cancer Drug Efficacy - Clinical OMICs News\\n\\nRead more here: https://t.co/9u9NStoHIr\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                         \n",
      "44  3 Questions: Artificial intelligence for health care equity - MIT News\\n\\nRead more here: https://t.co/bDrEJjAODD\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                                 \n",
      "45  CamerEye Introduces First Artificial Intelligence Smart Fence and Pool Safety Ecosystem for Faster Distress and Near-Drowning Detection to Help Save Lives - PRNewswire\\n\\nRead more here: https://t.co/KC5lJPhiPg\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #…                             \n",
      "46  Using Artificial Intelligence To Discover New Drug Treatments: Science Next - WRKF\\n\\nRead more here: https://t.co/ZFFZ2dOw8L\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                     \n",
      "47  AI in Drug Discovery Starts to Live Up to the Hype - Genetic Engineering &amp; Biotechnology News\\n\\nRead more here: https://t.co/ZC5the5gFA\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #Robots #IoT                                               \n",
      "48  Artificial Intelligence-Based Security Market Key Factor Drive Growth Is Increasing Adoption of Internet of Things - Rome News-Tribune\\n\\nRead more here: https://t.co/Vxz43UMQTY\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearni…                             \n",
      "49  Machine Learning Technique Reveals Cancer Genetic Insights - https://t.co/M8pNExXXMN\\n\\nRead more here: https://t.co/K6Dr9Vmg4A\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                                   \n",
      "50  Artificial Intelligence-Based Security Market Key Factor Drive Growth Is Increasing Adoption of Internet of Things - WV News\\n\\nRead more here: https://t.co/QAOyJTEphF\\n\\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #R…                             \n",
      "51  CamerEye Introduces First Artificial Intelligence Smart Fence and Pool Safety Ecosystem for Faster Distress and Near-Drowning Detection to Help Save Lives - PRNewswire\\n\\nRead more here: https://t.co/nXnVDvfUJP\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #…                             \n",
      "52  The Titanoboa robotic snake is 15 meters long.\\nBy @futurism \\n#Robotics #Robot #AI #Engineering #MachineLearning #Tech #IoT #5G\\n#ArtificialIntelligence #innovation\\nCc: @Nicochan33 @sebbourguignon @labordeolivier @LouisSerge @gvalan @DrJDrooghaag @mvollmer1 @RLDI_Lamy @Victoryabro https://t.co/BtG1fe6adE     \n",
      "53  Artificial Intelligence-Based Security Market Key Factor Drive Growth Is Increasing Adoption of Internet of Things - WV News\\n\\nRead more here: https://t.co/tEnC3QRuMt\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #100DaysOfCode #Python #DeepLearning #NLP #R…                             \n",
      "54  Flying #autonomous #robots that can pick fruits\\nby @mashable\\n\\n#Drones #Robotics #RPA #AI #ArtificialIntelligence #SupplyChain #Tech #Technology\\n\\nCc: @paulg @vanrijmenam @davemcclure https://t.co/sFPrvJciQ0                                                                                                      \n",
      "55  Aspinity's Analog Machine Learning Drives Near-Zero-Power Acoustic Event Detection - GlobeNewswire\\n\\nRead more here: https://t.co/t6LiRW3qXJ\\n\\n#ArtificialIntelligence #AI #DataScience #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT                                                                     \n",
      "56  With a long reach and small footprint\\nBy @BostonDynamics\\n#Robotics #Industry40 #IoT #IIoT\\n#AI #Tech #5G #innovation  #MachineLearning #ArtificialIntelligence\\nCc: @JolaBurnett @HaroldSinnott\\n@labordeolivier @sebbourguignon @Nicochan33 @mvollmer1 @LouisSerge @baski_LA @DrJDrooghaag https://t.co/QUzlbHjgdX   \n",
      "57  A friend shared this\\n\\n#AI #joke with me 😂\\n\\n#fintech #ArtificialIntelligence #MachineLearning @DimDrandakis @Xbond49 @Paula_Piccard @pierrepinna @terence_mills @HaroldSinnott @Ronald_vanLoon @YuHelenYu @Fisher85M @KMcDTech @ahier @DioFavatas https://t.co/SNUpFA1J39                                            \n",
      "58  Dr Anand Gupta, ADG- Development @DghIndia to join for the 2nd edition of Global Upstream Technology Conference #GUTEC2021\\n#GUTEC #artificialintelligence #blockchain #technology #bigdata #machinelearning #cloudcomputing #cybersecurity #edgecomputing #robotics #drones #registernow https://t.co/YpTciuZvMz       \n",
      "59  Highly recommended: \"AI virtues: The missing link in putting AI ethics into practice\"\\n\\nby Dr. Thilo Hagendorff\\n\\n#artificialintelligence #AIEthics #ethics #businessethics #psychology #MachineLearning #DataScience \\n\\nhttps://t.co/zZeFxTwGJy                                                                     \n",
      "60  Microsoft open sources tool to use AI in simulated attacks https://t.co/frdufS9Xs6 #AI #MachineLearning #DataScience #ArtificialIntelligence\\n\\nTrending AI/ML Article Identified &amp; Digested via Granola; a Machine-Driven RSS Bot by Ramsey Elbasheer https://t.co/YMjrp1gGU9                                      \n",
      "61  A checklist to track your Data Science progress https://t.co/mEaaanJHHM #AI #MachineLearning #DataScience #ArtificialIntelligence\\n\\nTrending AI/ML Article Identified &amp; Digested via Granola; a Machine-Driven RSS Bot by Ramsey Elbasheer https://t.co/ecxvoee6Zr                                                 \n",
      "62  Artificial Intelligence: The Last Human Invention https://t.co/Qj5yRP5FW5 #AI #MachineLearning #DataScience #ArtificialIntelligence\\n\\nTrending AI/ML Article Identified &amp; Digested via Granola; a Machine-Driven RSS Bot by Ramsey Elbasheer https://t.co/DIpkowmH4A                                               \n",
      "63  Detect abnormal equipment behavior and review predictions using Amazon Lookout for Equipment and Amazon A2I https://t.co/K7j1lCSIdm #AI #MachineLearning #DataScience #ArtificialIntelligence\\n\\nTrending AI/ML Article Identified &amp; Digested via Granola; a Machine-Driven RSS Bot by R… https://t.co/5V7zUNs1NY   \n",
      "64  Acoustic anomaly detection using Amazon Lookout for Equipment https://t.co/I93YfZhdal #AI #MachineLearning #DataScience #ArtificialIntelligence\\n\\nTrending AI/ML Article Identified &amp; Digested via Granola; a Machine-Driven RSS Bot by Ramsey Elbasheer https://t.co/pKOXzMiQoC                                   \n",
      "65  #Robots They’re just like us.\\nBy @cheddar\\n#AI #artwork #Art #arte #Robotics #IoT #5G #ArtificialIntelligence #MachineLearning #innovation #Tech #Technology\\nCc @Nicochan33 @jblefevre60 @chboursin @sebbourguignon @labordeolivier @Ym78200 @DrJDrooghaag @mvollmer1 @kalydeoo @baski_LA https://t.co/AVBTsiL5ZR     \n",
      "66  50+ Examples of How Blockchains are Taking Over the World via Dr Marcell Vollmer\\n\\n#blockchain #DistributedLedger #DLT #InternetOfThings #IoT #Industry40 #4IR #5G #SupplyChain #healthtech #Procurement #ArtificialIntelligence #AI #BigData #Analytics https://t.co/BdQu4ALRVP                                       \n",
      "67  These AI Functions Can identify Drug Abuse Risk: Says Studies https://t.co/ATpciAPBt0 #ArtificialIntelligence via @cybersecboardrm                                                                                                                                                                                      \n",
      "68  Lost Tapes of the 27 Club, a banging tune and inspiration, or fear, for future artists. Amazing story.\\n#nirvana #AI #artificialintelligence #futureofmusic???\\n\\nListen to 'new' Nirvana song 'Drowned In The Sun' created using AI https://t.co/fc8gbcM5MI                                                            \n",
      "69  Anduril Is About To Give An AI Brain Transplant To Area-I’s Drones @Forbes #AI #ArtificialIntelligence #startup #tech #innovation #data #ML #MachineLearning #drones #robotics #100DaysOfCode #femtech #DEVCommunity #WomenWhoCode #DigitalTransformation  https://t.co/TTs73Q1QMK                                      \n",
      "70  🔋⚡French battery ecosystem @powerup_xyz @DraculaTech @ForseePower @TiamatEnergy @wattalps @OTONOHM @Saft_batteries @umatech #solidstatebatteries #ArtificialIntelligence https://t.co/kyyCgcowRp                                                                                                                        \n",
      "71  \"How Artificial Intelligence Tech Emulated Kurt Cobain to Release a New Nirvana Song\" by @vocal_creators\\n Listen to \"Drowned in the sun.\\n #artificialintelligence #algorithms #datascience  #AI #Nirvana #bigdata #KurtCobain #technews  https://t.co/MNl9qZQLBE                                                      \n",
      "72  Quantitative evaluation of a pre-trained BERT model https://t.co/gFhqPnV9sP #AI #MachineLearning #DataScience #ArtificialIntelligence\\n\\nTrending AI/ML Article Identified &amp; Digested via Granola; a Machine-Driven RSS Bot by Ramsey Elbasheer https://t.co/12Ttmnggdz                                             \n",
      "73  27 years have passed since we lost Kurt Cobain at age 27. But now an AI algorithm has emulated his musical insight and recorded a new Nirvana song. Listen to \"Drowned in the sun\" #Nirvana #AI #technology\\n@Nirvana @NirvanaFans \\nhttps://t.co/ImeJKiKZ8b                                                            \n",
      "74  On Demand talk from Dr. Maryellen Giger - @UCCancerCenter  leader in #artificialintelligence and imaging in #breastcancer at #AACR21                                                                                                                                                                                    \n",
      "75  BionicSoftHand\\nBy @CNET\\nTY, MT @MargaretSiegien\\n\\nv/ @HeinzVHoenen\\n#Bionicle #AI #Robotics #robots #RPA #NLP #ArtificialIntelligence #ML #MachineLearning #DeepLearning #DL #IOT #IIoT #IoTPL #Analytics #DatsScience #BigData #CyberSecurity #5G #Serverless #CloudComputing #TechTrends https://t.co/wKcDr8EMfb   \n",
      "76  Ep 2. A.I. in the Driver’s Seat - WHYY https://t.co/eZHaORRfX1 #ArtificialIntelligence #TechJunkieNews https://t.co/DnqVuEiUQA                                                                                                                                                                                          \n",
      "\n",
      "             created_at  #quote  #reply  #retweet  #favorite  \n",
      "5   2021-03-10 08:53:14  0       1       538       58         \n",
      "6   2021-03-11 08:38:13  0       0       536       59         \n",
      "7   2021-03-20 08:38:19  0       0       535       56         \n",
      "8   2021-03-19 07:38:15  1       0       513       48         \n",
      "9   2021-03-20 08:53:36  0       0       493       48         \n",
      "10  2021-03-12 16:04:10  2       0       392       30         \n",
      "11  2021-03-15 07:04:03  0       1       387       37         \n",
      "12  2021-03-11 08:43:33  1       0       374       28         \n",
      "13  2021-03-10 22:43:32  0       0       371       29         \n",
      "14  2021-03-11 12:43:20  0       0       367       27         \n",
      "15  2021-03-10 16:04:15  0       1       367       25         \n",
      "16  2021-03-12 08:04:32  0       0       364       26         \n",
      "17  2021-03-19 07:43:31  0       0       362       29         \n",
      "18  2021-03-11 23:23:30  0       0       359       23         \n",
      "19  2021-03-17 07:04:15  0       0       357       32         \n",
      "20  2021-03-11 08:04:07  0       0       353       31         \n",
      "21  2021-03-17 14:04:18  0       0       353       38         \n",
      "22  2021-03-14 15:04:06  0       0       350       30         \n",
      "23  2021-03-19 07:04:41  0       0       349       33         \n",
      "24  2021-03-10 16:43:20  0       0       348       24         \n",
      "25  2021-03-20 07:43:42  0       0       347       31         \n",
      "26  2021-03-24 13:53:26  0       0       343       37         \n",
      "27  2021-03-11 13:04:27  0       0       339       28         \n",
      "28  2021-03-12 08:04:38  0       0       339       33         \n",
      "29  2021-03-10 13:04:16  0       0       333       31         \n",
      "30  2021-03-11 23:37:33  0       0       329       23         \n",
      "31  2021-03-15 21:04:45  0       0       326       27         \n",
      "32  2021-03-11 13:43:21  0       1       322       26         \n",
      "33  2021-03-16 10:04:17  0       0       321       29         \n",
      "34  2021-03-12 09:43:07  0       0       315       28         \n",
      "35  2021-03-24 13:38:11  0       0       314       33         \n",
      "36  2021-03-22 13:53:12  0       0       310       39         \n",
      "37  2021-03-15 19:04:50  2       0       306       25         \n",
      "38  2021-03-20 18:04:35  0       0       295       25         \n",
      "39  2021-04-01 07:38:07  0       0       292       38         \n",
      "40  2021-04-01 07:53:33  0       0       284       42         \n",
      "41  2021-04-02 09:38:15  0       0       284       38         \n",
      "42  2021-04-02 09:53:09  0       0       255       31         \n",
      "43  2021-03-25 17:04:14  0       0       197       28         \n",
      "44  2021-03-23 20:43:20  0       0       170       24         \n",
      "45  2021-03-23 16:38:43  0       0       159       19         \n",
      "46  2021-04-01 00:43:18  0       0       154       22         \n",
      "47  2021-04-02 15:38:26  0       0       152       32         \n",
      "48  2021-03-30 08:38:22  0       0       150       26         \n",
      "49  2021-03-31 14:04:51  0       0       146       19         \n",
      "50  2021-03-30 07:53:22  0       0       97        29         \n",
      "51  2021-03-23 18:43:28  0       0       94        15         \n",
      "52  2021-04-09 17:20:46  5       4       78        107        \n",
      "53  2021-03-30 07:38:08  0       0       58        10         \n",
      "54  2021-04-03 06:58:28  2       0       34        77         \n",
      "55  2021-04-06 13:05:12  0       0       24        8          \n",
      "56  2021-04-09 04:31:02  0       0       21        22         \n",
      "57  2021-04-10 14:36:28  1       4       19        30         \n",
      "58  2021-04-08 10:47:09  1       0       9         5          \n",
      "59  2021-04-10 16:59:06  0       0       4         3          \n",
      "60  2021-04-10 00:01:31  0       0       2         0          \n",
      "61  2021-04-10 00:01:30  0       0       2         0          \n",
      "62  2021-04-10 00:01:25  0       0       2         0          \n",
      "63  2021-04-09 19:51:33  0       0       2         0          \n",
      "64  2021-04-09 19:46:41  0       0       2         0          \n",
      "65  2021-04-10 17:25:53  0       0       0         0          \n",
      "66  2021-04-10 18:00:43  0       0       0         0          \n",
      "67  2021-04-10 18:25:02  0       0       0         0          \n",
      "68  2021-04-10 18:55:19  0       0       0         0          \n",
      "69  2021-04-10 18:57:59  0       0       0         0          \n",
      "70  2021-04-10 19:12:46  0       0       0         0          \n",
      "71  2021-04-10 19:17:12  0       0       0         0          \n",
      "72  2021-04-10 19:31:32  0       0       0         0          \n",
      "73  2021-04-10 19:46:57  0       0       0         0          \n",
      "74  2021-04-10 20:01:24  0       0       0         0          \n",
      "75  2021-04-10 20:17:58  0       0       0         0          \n",
      "76  2021-04-10 20:57:03  0       0       0         0          \n"
     ]
    }
   ],
   "source": [
    "search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.get(\"ArtificialIntelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclient.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Cache:\n",
    "    def __init__(self, capacity=10):\n",
    "        self._capacity = capacity\n",
    "        self._dict = {}\n",
    "        self._queue = deque([])\n",
    "    \n",
    "    def queue_update(self, key):\n",
    "        #update queue\n",
    "        if key in self._queue:\n",
    "            self._queue.remove(key)\n",
    "        self._queue.append(key)\n",
    "    \n",
    "    def get(self, key):\n",
    "        self.queue_update(key)\n",
    "        return self._dict.get(key, -1) # return -1 if not in cache\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        \n",
    "        if key in self._dict.keys(): #if already exist\n",
    "            self.queue_update(key)\n",
    "        elif len(self._dict)>=self._capacity: #if not exist but over the capacity\n",
    "            LRU = self._queue.popleft()\n",
    "            del self._dict[LRU]\n",
    "        else: #new key value pair\n",
    "            self._queue.append(key)\n",
    "            self._dict[key] = value\n",
    "    \n",
    "    def clean(self):\n",
    "        self._dict.clear()\n",
    "        \n",
    "    def print_keys(self):\n",
    "        keys = self._dict.keys()\n",
    "        return keys\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.print_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
